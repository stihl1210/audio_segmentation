{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset preprocessing\n",
    "\n",
    "https://zenodo.org/record/1302992"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls /Users/rafalpilarczyk/Downloads/ismir04_genre/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = '/Users/rafalpilarczyk/Downloads/ismir04_genre/audio'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_list(directory):\n",
    "    ls = []\n",
    "    for root, _, files in os.walk(directory, topdown=False):\n",
    "        for name in files:\n",
    "            ls.append(os.path.join(root, name))\n",
    "    ls = list(filter(lambda x : x.split('/')[-1][0] !='.', ls)) #removing .start_file :)\n",
    "    return ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "train_dir = os.path.join(DATASET_DIR, 'training')\n",
    "eval_dir = os.path.join(DATASET_DIR, 'evaluation')\n",
    "\n",
    "classes = os.listdir(train_dir)\n",
    "classes = list(filter(lambda x : x[0] !='.', classes))\n",
    "\n",
    "\n",
    "cc = []\n",
    "\n",
    "def seconds_of_file(file_path):\n",
    "    try:\n",
    "        dr = librosa.get_duration(filename=file_path)\n",
    "        dr = int(dr)\n",
    "    except Exception as e:\n",
    "        print(f\"error with{file_path}\")\n",
    "        dr = 0\n",
    "    return dr\n",
    "\n",
    "for _cls in classes:\n",
    "    print(_cls)\n",
    "    files_dir = os.path.join(train_dir, _cls)\n",
    "    files_list = get_files_list(files_dir)\n",
    "    cc.extend([{'file_name' : file_name, 'class' : _cls, 'train' : True, 'seconds' : seconds_of_file(file_name)} for file_name in files_list])\n",
    "    \n",
    "    \n",
    "    files_dir = os.path.join(eval_dir, _cls)\n",
    "    files_list = get_files_list(files_dir)\n",
    "    cc.extend([{'file_name' : file_name, 'class' : _cls, 'train' : False, 'seconds' : seconds_of_file(file_name)} for file_name in files_list])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(cc)\n",
    "df.head()\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['seconds']>10]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['train'] == True].to_csv('training.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['train'] == False].to_csv('testing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df[df['train'] == False]['file_name'].iloc[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, fs = librosa.core.load(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.waveplot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "ipd.Audio(sample) # load a local WAV file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 5))\n",
    "D = librosa.amplitude_to_db(np.abs(librosa.stft(data)), ref=np.max)\n",
    "librosa.display.specshow(D, y_axis='linear')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Linear-frequency power spectrogram')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch loaders and class helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LibrosaLoader(object):\n",
    "    def __init__(self, duration=3):\n",
    "        self.duration = duration\n",
    "    \n",
    "    def __call__(self, path, max_size):\n",
    "        start = np.random.randint(0, max_size-self.duration-1)\n",
    "        data, _ = librosa.load(path, dtype=np.float32, duration=self.duration, offset=start)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STFT(object):\n",
    "    \n",
    "    def __init__(self, fft=1024, sr=16000, hop=160, win_length=400, clip=True):\n",
    "        self.size = fft\n",
    "        self.sr = sr\n",
    "        self.hop = hop\n",
    "        self.win_l = win_length\n",
    "        self.clip = clip\n",
    "        \n",
    "    def __call__(self, audio_file):\n",
    "        \n",
    "        if len(audio_file.shape) == 1:\n",
    "            audio_file = np.expand_dims(audio_file, axis=0)\n",
    "        return self._stft(audio_file[0, :])\n",
    "    \n",
    "    def _stft(self, audio_file):\n",
    "        y = np.abs(librosa.stft(audio_file, hop_length=self.hop, n_fft=self.size, win_length=self.win_l))\n",
    "        if self.clip:\n",
    "            y = y[1:513, :300] #remove last two elements\n",
    "        else:\n",
    "            y = y[1:513, :]\n",
    "            \n",
    "        y = (y - y.mean()) / y.std() #standarization in runtime\n",
    "        return y\n",
    "    \n",
    "class ArrayToTensor(object):\n",
    "    \n",
    "    def __call__(self, audio_file):\n",
    "        ten = torch.from_numpy(audio_file).float()\n",
    "        ten.unsqueeze_(0)\n",
    "        return ten\n",
    "    \n",
    "    def __repr__(self):\n",
    "        format_string = self.__class__.__name__\n",
    "        return format_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cls_to_idx_dict(list_of_speakers):\n",
    "    list_of_speakers = sorted(list_of_speakers)\n",
    "    class_to_idx = {list_of_speakers[i]: i for i in range(len(list_of_speakers))}\n",
    "    return class_to_idx\n",
    "\n",
    "\n",
    "class AudioSegmentBaseDataset(Dataset):\n",
    "    def __init__(self, csv_file, loader, transform=None, sample_audio=None):\n",
    "        self.dataset_df = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.loader = loader\n",
    "        \n",
    "        self._size_df = len(self.dataset_df)\n",
    "        \n",
    "        self.classes = list(self.dataset_df['class'].unique()) \n",
    "        self.class_to_idx = cls_to_idx_dict(self.classes)\n",
    "        print(self.class_to_idx)\n",
    "\n",
    "    def __getitem__(self, indice):\n",
    "        \n",
    "        sample_index = np.random.randint(0, self._size_df-1)\n",
    "        \n",
    "        row = self.dataset_df.iloc[sample_index]\n",
    "        \n",
    "        selected_class = row['class']\n",
    "        audio_name = row['file_name']\n",
    "        length = row['seconds']\n",
    "        \n",
    "\n",
    "        \n",
    "        cls = self.class_to_idx[selected_class]\n",
    "        audio = self.loader(audio_name, length)\n",
    "\n",
    "        if self.transform:\n",
    "            audio = self.transform(audio)\n",
    "\n",
    "        return audio, cls\n",
    "    \n",
    "    def __len__(self):\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trs_train = transforms.Compose([STFT(clip=False), ArrayToTensor()])\n",
    "\n",
    "ds_train = AudioSegmentBaseDataset('training.csv', loader=LibrosaLoader(duration=3.71), transform=trs_train)\n",
    "print(len(ds_train))\n",
    "ds_test = AudioSegmentBaseDataset('testing.csv', loader=LibrosaLoader(duration=3.71), transform=trs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train[0][0].unsqueeze(0).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "from torchvision.models.resnet import ResNet\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class inception_modified(models.Inception3):\n",
    "\n",
    "    def __init__(self, num_classes=8):\n",
    "        self.inplanes = 64\n",
    "        super(inception_modified, self).__init__(num_classes=num_classes, aux_logits=False)\n",
    "        self.Conv2d_1a_3x3 = BasicConv2d(1, 32, kernel_size=3, stride=2)\n",
    "        \n",
    "        self.emb = nn.Linear(2048, 128)\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        emb = F.relu(self.extract_embedding(x), inplace=True)\n",
    "        out = self.fc(emb)\n",
    "        return out\n",
    "        \n",
    "    def extract_embedding(self, x):\n",
    "        if self.transform_input:\n",
    "            x_ch0 = torch.unsqueeze(x[:, 0], 1) * (0.229 / 0.5) + (0.485 - 0.5) / 0.5\n",
    "            x_ch1 = torch.unsqueeze(x[:, 1], 1) * (0.224 / 0.5) + (0.456 - 0.5) / 0.5\n",
    "            x_ch2 = torch.unsqueeze(x[:, 2], 1) * (0.225 / 0.5) + (0.406 - 0.5) / 0.5\n",
    "            x = torch.cat((x_ch0, x_ch1, x_ch2), 1)\n",
    "        # 299 x 299 x 3\n",
    "        x = self.Conv2d_1a_3x3(x)\n",
    "        # 149 x 149 x 32\n",
    "        x = self.Conv2d_2a_3x3(x)\n",
    "        # 147 x 147 x 32\n",
    "        x = self.Conv2d_2b_3x3(x)\n",
    "        # 147 x 147 x 64\n",
    "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "        # 73 x 73 x 64\n",
    "        x = self.Conv2d_3b_1x1(x)\n",
    "        # 73 x 73 x 80\n",
    "        x = self.Conv2d_4a_3x3(x)\n",
    "        # 71 x 71 x 192\n",
    "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "        # 35 x 35 x 192\n",
    "        x = self.Mixed_5b(x)\n",
    "        # 35 x 35 x 256\n",
    "        x = self.Mixed_5c(x)\n",
    "        # 35 x 35 x 288\n",
    "        x = self.Mixed_5d(x)\n",
    "        # 35 x 35 x 288\n",
    "        x = self.Mixed_6a(x)\n",
    "        # 17 x 17 x 768\n",
    "        x = self.Mixed_6b(x)\n",
    "        # 17 x 17 x 768\n",
    "        x = self.Mixed_6c(x)\n",
    "        # 17 x 17 x 768\n",
    "        x = self.Mixed_6d(x)\n",
    "        # 17 x 17 x 768\n",
    "        x = self.Mixed_6e(x)\n",
    "        # 17 x 17 x 768\n",
    "        if self.training and self.aux_logits:\n",
    "            aux = self.AuxLogits(x)\n",
    "        # 17 x 17 x 768\n",
    "        x = self.Mixed_7a(x)\n",
    "        # 8 x 8 x 1280\n",
    "        x = self.Mixed_7b(x)\n",
    "        # 8 x 8 x 2048\n",
    "        x = self.Mixed_7c(x)\n",
    "        # 8 x 8 x 2048\n",
    "        x = F.avg_pool2d(x, kernel_size=8)\n",
    "        # 1 x 1 x 2048\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        # 1 x 1 x 2048\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # 2048\n",
    "        x = self.emb(x)\n",
    "        # 1000 (num_classes)\n",
    "        if self.training and self.aux_logits:\n",
    "            return x, aux\n",
    "        return x\n",
    "        \n",
    "net = inception_modified()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.forward(ds_train[0][0].unsqueeze(0)) #forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.extract_embedding(ds_train[0][0].unsqueeze(0)).size() #embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(net, data_val_loader, device, criterion, cuda=False):\n",
    "    with torch.no_grad():\n",
    "        loss_sum = 0\n",
    "        total = 0\n",
    "        correct_top1 = 0\n",
    "        correct_top5 = 0\n",
    "        for batch_idx, (inputs, targets) in tqdm(enumerate(data_val_loader)):\n",
    "            if cuda:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            prec_1, prec_5 = accuracy_top1_5(outputs, targets, topk=(1, 5))\n",
    "            correct_top1 += prec_1\n",
    "            correct_top5 += prec_5\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss_sum += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            # correct += predicted.eq(targets).sum().item()\n",
    "    return loss_sum, correct_top1, correct_top5, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds_for_workers(worker_no):\n",
    "    np.random.seed()\n",
    "    \n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def accuracy_top1_5(output, target, topk=(1,)):\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k)\n",
    "    return res\n",
    "\n",
    "def show_metrics(prefix, index, size, loss_sum, performance):\n",
    "    print(prefix + \"Iter %d, size %d, loss %6.6f, performance %3.2f percent\" % (\n",
    "        index, size, loss_sum, 100.0 * performance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(cuda=False, epochs=50):\n",
    "    net = inception_modified()\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Selected device {} \".format(device))\n",
    "  \n",
    "    print(\"Cuda devices found {}\".format(torch.cuda.device_count()))\n",
    "    if cuda:\n",
    "        net = net.cuda()\n",
    "        cudnn.benchmark = True\n",
    "\n",
    "    \n",
    "    net.to(device)\n",
    "   \n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.1)\n",
    "    #optimizer = optim.Adam(net.parameters(), lr=args.lr) # you can also use Adam\n",
    "    criterion = torch.nn.CrossEntropyLoss() \n",
    "    if cuda:\n",
    "        criterion = criterion.cuda()\n",
    "        print(\"Criterion {} \".format(criterion))\n",
    "        \n",
    "    data_train_loader = DataLoader(ds_train, 1,\n",
    "                                        num_workers=1,\n",
    "                                        pin_memory=True, worker_init_fn=set_seeds_for_workers)\n",
    "    data_val_loader = DataLoader(ds_test, 1,\n",
    "                                      num_workers=1,\n",
    "                                      pin_memory=True, drop_last=True, worker_init_fn=set_seeds_for_workers)\n",
    "    \n",
    "    print(len(ds_train), len(ds_test))\n",
    "    \n",
    "    number_of_epochs = epochs\n",
    "    loss_sum = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    iteration = 0\n",
    "\n",
    "    for epoch in range(0, number_of_epochs):\n",
    "        net.train()\n",
    "        \n",
    "        start_epoch_time = time.time()\n",
    "        losses = AverageMeter() #metrics for loss, acc\n",
    "        accs = AverageMeter()\n",
    "        \n",
    "        loss_sum = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(data_train_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            \n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_batch = loss.item()\n",
    "            loss_sum += loss_batch\n",
    "            _, predicted = outputs.max(1)\n",
    "            total_batch = targets.size(0)\n",
    "            total += total_batch\n",
    "            correct_batch = predicted.eq(targets).sum().item()\n",
    "            correct += correct_batch\n",
    "            \n",
    "            losses.update(loss.item(), inputs.size(0))\n",
    "            accs.update(correct_batch / total_batch, inputs.size(0))\n",
    "            show_metrics(\"Batch top 1: \", batch_idx,1, loss_batch, correct_batch / total_batch)\n",
    "            \n",
    "            iteration += 1\n",
    "            \n",
    "        stop_epoch_time = time.time() - start_epoch_time\n",
    "        torch.save(net.state_dict(), output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc-showcode": true,
  "toc-showmarkdowntxt": true,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
